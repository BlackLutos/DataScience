{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\V2X\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\V2X\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/user/.cache/huggingface/datasets/json/default-ca6af2841e934a3b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 42.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"train.json\")\n",
    "body_text = dataset['train']['body']\n",
    "title_text = dataset['train']['title']\n",
    "input_text = []\n",
    "summary = []\n",
    "data_size = 100000\n",
    "for body in range(data_size):\n",
    "    input_text.append('summarize: ' + str(body_text[body]))\n",
    "for title in range(data_size):\n",
    "    summary.append(str(title_text[title]))\n",
    "\n",
    "input_train, input_val, summary_train, summary_val = train_test_split(input_text, summary, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineGenerationDataset(Dataset):\n",
    "    def __init__(self, text, summary, tokenizer, max_len = 512):\n",
    "        self.data = []\n",
    "        for t, s in zip(text, summary):\n",
    "            input_t = tokenizer(t, truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "            label_t = tokenizer(s, truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "\n",
    "            #轉換-100\n",
    "            for cnt,tmp in enumerate(label_t['input_ids']):\n",
    "                if tmp == 0:\n",
    "                    label_t['input_ids'][cnt] = -100\n",
    "                    \n",
    "            self.data.append({'input_ids':torch.tensor(input_t['input_ids']),\n",
    "                              'attention_mask':torch.tensor(input_t['attention_mask']),\n",
    "                              'labels':torch.tensor(label_t['input_ids'])})\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "         \n",
    "        return self.data[index]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HeadlineGenerationDataset(input_train, summary_train, tokenizer,max_len = 512)\n",
    "train_loader = DataLoader(train_set,batch_size = 2,shuffle = True, num_workers = 0, pin_memory = True)\n",
    "val_set = HeadlineGenerationDataset(input_val, summary_val, tokenizer,max_len = 512)\n",
    "val_loader = DataLoader(val_set,batch_size = 2,shuffle = True, num_workers = 0, pin_memory = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valdation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valdation(val_loader, model, tokenizer):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].cuda()\n",
    "            attention_mask = batch['attention_mask'].cuda()\n",
    "            labels = batch['labels'].cuda()\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            for i in range(len(outputs)):\n",
    "                output = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "                preds.append(output)\n",
    "                label = labels[i].cpu().numpy()\n",
    "                label = label[label != -100]  # filter out padding labels\n",
    "                targets.append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "    return preds, targets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def evaluation(outputs, targets):\n",
    "    metric_rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"])\n",
    "    rouge = metric_rouge.compute(predictions=outputs, references=targets, use_stemmer=True)\n",
    "    return rouge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 45000/45000 [5:28:51<00:00,  2.28it/s, Loss=1.93]      \n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\V2X\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5000/5000 [22:59<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.4383037130718228, 'rouge2': 0.25733217517877927, 'rougeL': 0.40248471906622996, 'rougeLsum': 0.4025951650614007}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 45000/45000 [5:23:41<00:00,  2.32it/s, Loss=1.84]      \n",
      "100%|██████████| 5000/5000 [21:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.4478721153096462, 'rouge2': 0.26634157589627416, 'rougeL': 0.41275185668574155, 'rougeLsum': 0.41256062064590165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 45000/45000 [5:21:39<00:00,  2.33it/s, Loss=1.38]      \n",
      "100%|██████████| 5000/5000 [21:08<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.45478295791469847, 'rouge2': 0.27161383269310724, 'rougeL': 0.41781696653018463, 'rougeLsum': 0.41804808595261844}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 45000/45000 [5:21:40<00:00,  2.33it/s, Loss=1.55]      \n",
      "100%|██████████| 5000/5000 [21:24<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.45993916006354185, 'rouge2': 0.27430596267822527, 'rougeL': 0.42164516271097885, 'rougeLsum': 0.4217303176463495}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 45000/45000 [5:25:42<00:00,  2.30it/s, Loss=0.768]     \n",
      "100%|██████████| 5000/5000 [22:30<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.4622527862773976, 'rouge2': 0.2788786859745255, 'rougeL': 0.4245154737074517, 'rougeLsum': 0.4246260630353029}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 45000/45000 [5:26:47<00:00,  2.30it/s, Loss=1.33]      \n",
      "100%|██████████| 5000/5000 [20:13<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.4659912351844707, 'rouge2': 0.2829143937043717, 'rougeL': 0.4288290651995629, 'rougeLsum': 0.42884417264946995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 45000/45000 [5:22:03<00:00,  2.33it/s, Loss=0.892]     \n",
      "100%|██████████| 5000/5000 [20:59<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.4641527924506976, 'rouge2': 0.282577865960329, 'rougeL': 0.4277944873590433, 'rougeLsum': 0.4280565851014666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 45000/45000 [5:21:41<00:00,  2.33it/s, Loss=1.89]      \n",
      "100%|██████████| 5000/5000 [20:55<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.46440037254909167, 'rouge2': 0.28339393737741037, 'rougeL': 0.42864769094796507, 'rougeLsum': 0.4285585096304675}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 45000/45000 [5:25:16<00:00,  2.31it/s, Loss=0.668]     \n",
      "100%|██████████| 5000/5000 [21:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.46214239154382775, 'rouge2': 0.28171389806537184, 'rougeL': 0.4259167077192326, 'rougeLsum': 0.42579484142673707}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 45000/45000 [5:21:31<00:00,  2.33it/s, Loss=0.61]      \n",
      "100%|██████████| 5000/5000 [21:27<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores:  {'rouge1': 0.45998606430860156, 'rouge2': 0.2802696819136486, 'rougeL': 0.42387975099678343, 'rougeLsum': 0.4236974574441758}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model.cuda()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = 1e-4)\n",
    "if os.path.exists('checkpoint.pth'):\n",
    "    checkpoint = torch.load('checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train = tqdm(train_loader)\n",
    "    for data in train:\n",
    "        for key in data.keys():\n",
    "            data[key] = data[key].cuda()\n",
    "        outputs = model(**data)\n",
    "        loss = outputs.loss\n",
    "        train.set_description(f'Epoch {epoch+1}')\n",
    "        train.set_postfix({'Loss': loss.item()})\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "    outputs, targets = valdation(val_loader, model, tokenizer)\n",
    "    rouge = evaluation(outputs, targets)\n",
    "    print(\"Rouge scores: \" , rouge)\n",
    "        \n",
    "    \n",
    "    model.save_pretrained('model_{}'.format(epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "V2X",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
